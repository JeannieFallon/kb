# Python

> Python is a clear and powerful object-oriented programming language, comparable to Perl, Ruby, Scheme, or Java.
> Some of Python's notable features:
> - Uses an elegant syntax, making the programs you write easier to read.
> - Is an easy-to-use language that makes it simple to get your program working. This makes Python ideal for prototype development and other ad-hoc programming tasks, without compromising maintainability.
> - Comes with a large standard library that supports many common programming tasks such as connecting to web servers, searching text with regular expressions, reading and modifying files.

https://wiki.python.org/moin/BeginnersGuide/Overview

## Official Documentation
Language Specification: \
https://docs.python.org/3/

PEP 8 Style Guide: \
https://peps.python.org/pep-0008/

Installation and Set-up: \
https://www.python.org/downloads/

## Developer Tools

PyCharm: \
https://www.jetbrains.com/pycharm/download/

Anaconda: \
https://www.anaconda.com/

Jupyter Notebook: \
https://jupyter.org/

## Package Management

Python Package Index (PyPI) is the official Python package repository: \
https://pypi.org/

`pip` is the official Python package installer: \
https://pip.pypa.io/en/latest/

Package installation instructions: \
https://packaging.python.org/en/latest/tutorials/installing-packages/

### Popular Packages

#### Web Applications

Flask: \
https://pypi.org/project/Flask/

Django: \
https://pypi.org/project/Django/

#### Asynchronous Functions

asyncio: \
https://pypi.org/project/asyncio/

#### Data Validation

Pydantic: \
https://pypi.org/project/pydantic/

#### Web Scraping

Beautiful Soup: \
https://pypi.org/project/beautifulsoup4/

## Deep Dive

The following was generated by Claude as a targeted review for Python 3.6+.

### 1. Time Complexities for Core Data Structures

#### Lists (Dynamic Arrays)
```python
# Access by index: O(1)
my_list = [1, 2, 3, 4, 5]
print(my_list[2])  # O(1)

# Append to end: O(1) amortized
my_list.append(6)  # O(1)

# Insert at beginning/middle: O(n)
my_list.insert(0, 0)  # O(n) - shifts all elements

# Delete by index: O(n) for beginning/middle, O(1) for end
del my_list[0]  # O(n) - shifts all elements
my_list.pop()   # O(1) - removes from end

# Search: O(n)
if 3 in my_list:  # O(n) - linear search
    print("Found")

# Sort: O(n log n)
my_list.sort()  # O(n log n)
```

#### Dictionaries (Hash Tables)
```python
# Access, insert, delete: O(1) average case, O(n) worst case
my_dict = {'a': 1, 'b': 2}
value = my_dict['a']     # O(1)
my_dict['c'] = 3         # O(1)
del my_dict['b']         # O(1)

# Membership test: O(1) average
if 'a' in my_dict:       # O(1)
    print("Key exists")

# Iterate over keys/values: O(n)
for key in my_dict:      # O(n)
    print(key)
```

#### Sets (Hash Tables)
```python
# Add, remove, membership: O(1) average case
my_set = {1, 2, 3}
my_set.add(4)            # O(1)
my_set.remove(2)         # O(1)
if 3 in my_set:          # O(1)
    print("Found")

# Set operations
set1 = {1, 2, 3}
set2 = {3, 4, 5}
union = set1 | set2      # O(len(set1) + len(set2))
intersection = set1 & set2  # O(min(len(set1), len(set2)))
```

#### Tuples (Immutable Sequences)
```python
# Access by index: O(1)
my_tuple = (1, 2, 3, 4)
print(my_tuple[1])       # O(1)

# Search: O(n)
if 3 in my_tuple:        # O(n)
    print("Found")

# No modification operations (immutable)
# Converting to list for modifications: O(n)
new_list = list(my_tuple)  # O(n)
```

### 2. Iterators and Generators

#### Understanding Iterators
```python
# Every iterable has an iterator
my_list = [1, 2, 3]
iterator = iter(my_list)

print(next(iterator))  # 1
print(next(iterator))  # 2
print(next(iterator))  # 3
# print(next(iterator))  # StopIteration exception

# Custom iterator
class Counter:
    def __init__(self, max_count):
        self.max_count = max_count
        self.count = 0
    
    def __iter__(self):
        return self
    
    def __next__(self):
        if self.count < self.max_count:
            self.count += 1
            return self.count
        raise StopIteration

counter = Counter(3)
for num in counter:
    print(num)  # Prints 1, 2, 3
```

#### Understanding Generators
```python
# Generator function with yield
def fibonacci_generator(n):
    a, b = 0, 1
    count = 0
    while count < n:
        yield a
        a, b = b, a + b
        count += 1

# Memory efficient - generates values on demand
fib = fibonacci_generator(10)
for num in fib:
    print(num)

# Generator expression (like list comprehension but lazy)
squares = (x**2 for x in range(1000000))  # Memory efficient
squares_list = [x**2 for x in range(1000000)]  # Memory intensive

# Generator for reading large files
def read_large_file(file_path):
    with open(file_path, 'r') as file:
        for line in file:
            yield line.strip()

# When to use generators:
# 1. Processing large datasets that don't fit in memory
# 2. Infinite sequences
# 3. Pipeline processing
# 4. When you don't need all values at once
```

#### Generator Use Cases
```python
# Pipeline processing example
def read_numbers(filename):
    with open(filename) as f:
        for line in f:
            yield int(line.strip())

def square_numbers(numbers):
    for num in numbers:
        yield num ** 2

def filter_even(numbers):
    for num in numbers:
        if num % 2 == 0:
            yield num

# Chain generators efficiently
pipeline = filter_even(square_numbers(read_numbers('numbers.txt')))
for result in pipeline:
    print(result)
```

### 3. Collections Module Deep Dive

#### defaultdict
```python
from collections import defaultdict

# Automatically creates missing keys with default values
dd = defaultdict(list)
dd['fruits'].append('apple')  # No KeyError, creates empty list first
dd['fruits'].append('banana')

# Common use case: grouping
data = [('fruit', 'apple'), ('vegetable', 'carrot'), ('fruit', 'banana')]
grouped = defaultdict(list)
for category, item in data:
    grouped[category].append(item)
print(dict(grouped))  # {'fruit': ['apple', 'banana'], 'vegetable': ['carrot']}

# Different default factories
dd_int = defaultdict(int)      # Default value: 0
dd_set = defaultdict(set)      # Default value: empty set
dd_lambda = defaultdict(lambda: "N/A")  # Custom default
```

#### Counter
```python
from collections import Counter

# Count occurrences
text = "hello world"
counter = Counter(text)
print(counter)  # Counter({'l': 3, 'o': 2, 'h': 1, 'e': 1, ' ': 1, 'w': 1, 'r': 1, 'd': 1})

# Most common elements
print(counter.most_common(3))  # [('l', 3), ('o', 2), ('h', 1)]

# Counter arithmetic
c1 = Counter(['a', 'b', 'c', 'a'])
c2 = Counter(['a', 'b', 'b'])
print(c1 + c2)  # Counter({'a': 3, 'b': 3, 'c': 1})
print(c1 - c2)  # Counter({'c': 1, 'a': 1})

# Update counters
words = ['apple', 'banana', 'apple', 'cherry']
word_count = Counter()
word_count.update(words)
```

#### deque (Double-ended queue)
```python
from collections import deque

# Efficient operations at both ends
dq = deque([1, 2, 3])
dq.appendleft(0)    # O(1) - add to left
dq.append(4)        # O(1) - add to right
print(dq)           # deque([0, 1, 2, 3, 4])

left = dq.popleft()  # O(1) - remove from left
right = dq.pop()     # O(1) - remove from right

# Use cases: sliding window, breadth-first search
def sliding_window_max(nums, k):
    dq = deque()
    result = []
    
    for i, num in enumerate(nums):
        # Remove elements outside window
        while dq and dq[0] <= i - k:
            dq.popleft()
        
        # Remove smaller elements
        while dq and nums[dq[-1]] <= num:
            dq.pop()
        
        dq.append(i)
        
        if i >= k - 1:
            result.append(nums[dq[0]])
    
    return result
```

#### namedtuple
```python
from collections import namedtuple

# Create a class with named fields
Point = namedtuple('Point', ['x', 'y'])
p = Point(1, 2)
print(p.x, p.y)  # Access by name: 1 2
print(p[0], p[1])  # Access by index: 1 2

# Immutable and memory efficient
Person = namedtuple('Person', ['name', 'age', 'email'])
john = Person('John Doe', 30, 'john@email.com')

# Methods available
print(john._asdict())  # Convert to dictionary
print(john._replace(age=31))  # Create new instance with changes
print(Person._fields)  # Get field names

# Use cases: returning multiple values, configuration objects
def get_user_info():
    return Person('Alice', 25, 'alice@email.com')

def process_coordinates():
    points = [Point(1, 2), Point(3, 4), Point(5, 6)]
    return points
```

### 4. Shallow vs Deep Copying

#### Shallow Copy
```python
import copy

# Original list with nested objects
original = [[1, 2, 3], [4, 5, 6]]

# Shallow copy - creates new list but references same nested objects
shallow = copy.copy(original)
# or shallow = original.copy()
# or shallow = list(original)

# Modifying top-level doesn't affect original
shallow.append([7, 8, 9])
print(original)  # [[1, 2, 3], [4, 5, 6]] - unchanged

# But modifying nested objects affects both
shallow[0].append(999)
print(original)  # [[1, 2, 3, 999], [4, 5, 6]] - changed!
print(shallow)   # [[1, 2, 3, 999], [4, 5, 6], [7, 8, 9]]
```

#### Deep Copy
```python
import copy

original = [[1, 2, 3], [4, 5, 6]]

# Deep copy - creates completely independent copy
deep = copy.deepcopy(original)

# Modifying nested objects doesn't affect original
deep[0].append(999)
print(original)  # [[1, 2, 3], [4, 5, 6]] - unchanged
print(deep)      # [[1, 2, 3, 999], [4, 5, 6]]

# Complex object example
class Person:
    def __init__(self, name, friends):
        self.name = name
        self.friends = friends

original_person = Person("Alice", ["Bob", "Charlie"])
shallow_person = copy.copy(original_person)
deep_person = copy.deepcopy(original_person)

# Shallow copy shares the friends list
shallow_person.friends.append("David")
print(original_person.friends)  # ['Bob', 'Charlie', 'David']

# Deep copy has independent friends list
deep_person.friends.append("Eve")
print(original_person.friends)  # ['Bob', 'Charlie', 'David'] - unchanged
```

#### When to Use Each
```python
# Shallow copy: when you need independent top-level structure
# but can share nested objects (common with immutable nested objects)
matrix = [[1, 2], [3, 4]]
row_copy = matrix.copy()  # Safe if you only add/remove rows

# Deep copy: when you need complete independence
# (expensive operation, use sparingly)
complex_config = {
    'database': {'host': 'localhost', 'users': ['admin', 'guest']},
    'cache': {'enabled': True, 'servers': ['cache1', 'cache2']}
}
independent_config = copy.deepcopy(complex_config)
```

### 5. Memory Management and Garbage Collection

#### Reference Counting
```python
import sys

# Python tracks references to each object
a = [1, 2, 3]
print(sys.getrefcount(a))  # Shows reference count

b = a  # Increases reference count
print(sys.getrefcount(a))

del b  # Decreases reference count
print(sys.getrefcount(a))

# When reference count reaches 0, object is deallocated
```

#### Garbage Collection for Cycles
```python
import gc
import weakref

# Circular references can't be handled by reference counting alone
class Node:
    def __init__(self, value):
        self.value = value
        self.parent = None
        self.children = []
    
    def add_child(self, child):
        child.parent = self  # Circular reference!
        self.children.append(child)

# Create circular reference
parent = Node("parent")
child = Node("child")
parent.add_child(child)

# Even if we delete our references, the cycle remains
del parent, child

# Python's garbage collector handles this
print(f"Garbage objects collected: {gc.collect()}")

# Check garbage collection stats
print(gc.get_stats())
```

#### Memory Optimization Techniques
```python
# 1. Use __slots__ to reduce memory overhead
class Point:
    __slots__ = ['x', 'y']  # Prevents dynamic attribute creation
    
    def __init__(self, x, y):
        self.x = x
        self.y = y

# 2. Use generators instead of lists for large datasets
def process_large_dataset():
    # Memory efficient
    return (expensive_operation(x) for x in range(1000000))
    
    # Memory intensive alternative:
    # return [expensive_operation(x) for x in range(1000000)]

# 3. Use weakref to avoid circular references
import weakref

class Parent:
    def __init__(self):
        self.children = []
    
    def add_child(self, child):
        self.children.append(child)
        child.parent = weakref.ref(self)  # Weak reference

class Child:
    def __init__(self):
        self.parent = None
    
    def get_parent(self):
        if self.parent is not None:
            return self.parent()  # Call weak reference
        return None

# 4. Delete large objects when done
large_data = load_huge_dataset()
process_data(large_data)
del large_data  # Explicitly free memory
```

#### Memory Profiling Tips
```python
import tracemalloc

# Start tracing memory allocations
tracemalloc.start()

# Your code here
data = [i for i in range(100000)]

# Get current memory usage
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")

tracemalloc.stop()

# Use sys.getsizeof() for object size
import sys
print(f"Size of list: {sys.getsizeof(data)} bytes")
```

### Key Interview Takeaways

1. **Know your complexities**: Be able to explain why dict lookup is O(1) but list search is O(n)
2. **Generators save memory**: Use them for large datasets or when you don't need all values at once
3. **Collections module**: These tools solve common problems efficiently
4. **Copy carefully**: Understand when you need shallow vs deep copies
5. **Memory matters**: Python manages memory automatically, but understanding the basics helps with optimization

Practice implementing these concepts and be ready to explain trade-offs between different approaches!
